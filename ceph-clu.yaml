heat_template_version: 2017-02-24

description: Ceph cluster

parameters:
  admin_key_name:
    type: string
    label: Key Name
    description: Name of key-pair to be used for compute instance
    default: ssh_key_mgmt_admin
    
  public_net:
    type: string
    label: Public net
    description: Public net pool name
    default: ext-net
    
  mgmt_net:
    type: string
    label: Management net
    description: Management net pool name
    default: mgmt_internal_net
    
  volume_size:
    type: number
    label: Volume size
    description: Storage volumes size
    default: 15
    
  admin_username:
    type: string
    label: Admin username
    description: Admin username
    default: ubuntu
    
  admin_password:
    type: string
    label: Admin password
    description: Admin user initial pasword
    default: Admin123

resources:
  ssh_key_ceph_admin:
    type: OS::Nova::KeyPair
    properties:
      name: ssh_key_ceph_admin
      save_private_key: true
      
  ssh_key_mgmt_admin:
    type: OS::Nova::KeyPair
    external_id: ssh_key_mgmt_admin
    properties:
      save_private_key: true
      
      
      
  config_base:
    type: OS::Heat::SoftwareConfig
    properties:
      config:
        str_replace:
          template: |
            #!/bin/bash 
            apt update
            apt -y --no-install-recommends install ntp python3-pip docker.io
          params:
            $ssh_pub_key: { get_attr: [ ssh_key_ceph_admin, public_key ] }
        
      
  config_enable_root_ssh_login:
    type: OS::Heat::SoftwareConfig
    properties:
      config:
        str_replace:
          template: |
            #!/bin/bash
            echo "$ssh_pub_key" | tee /root/.ssh/authorized_keys
          params:
            $ssh_pub_key: { get_attr: [ ssh_key_ceph_admin, public_key ] }
           
  config_install_rgw_proxy:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config: 
        str_replace:
          template: |
            #!/bin/bash
            
            apt -y --no-install-recommends install nginx
            mkdir /mnt/rgw-cache
            echo "tmpfs    /mnt/rgw-cache   tmpfs   size=1g,uid=33,gid=33   0   0" | tee -a /etc/fstab
            mount -a
            
            cat <<EOF > /etc/nginx/sites-enabled/beast
            proxy_cache_path /mnt/rgw-cache levels=1:2 keys_zone=rgw-cache:10m inactive=24h max_size=1g;
            
            map \$sent_http_content_type \$expires {
                default                    6M;
                ~image/                    2w;
                ~video/                    1m;
            }
            
            upstream rgws {
                # List of all rgws (ips or resolvable names)
                server ceph-monitor-node01:8080 max_fails=2 fail_timeout=5s;
                server ceph-monitor-node02:8080 max_fails=2 fail_timeout=5s;
                server ceph-monitor-node03:8080 max_fails=2 fail_timeout=5s;
            }
            
            server {
                listen 8080;
                listen [::]:8080;
            
                server_name rgw;
            
                client_max_body_size 0;
            
                proxy_buffering off;
                proxy_set_header Host \$host;
                proxy_set_header X-Forwarded-For \$remote_addr;
            
                proxy_cache            rgw-cache;
                proxy_cache_valid      200 302  2d;
                proxy_cache_valid      any  1m;
                proxy_cache_min_uses   2;
                proxy_cache_use_stale  error timeout invalid_header updating http_500 http_502 http_503 http_504;
            
                expires \$expires;
            
                location / {
                    proxy_pass http://rgws;
                }
            }
            EOF
            
            systemctl reload nginx
            
          params:
            $ceph_release: 17.2.0

  config_for_admins:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config: 
        str_replace:
          template: |
            #!/bin/bash
            alias sec='echo; echo; echo "============================"; echo $*'
            
            sec "Adding admin private key to root"
            echo "$ssh_priv_key" | tee /root/.ssh/id_rsa
            chmod 600 /root/.ssh/id_rsa
            ssh-keygen -p -N "" -f /root/.ssh/id_rsa
            ssh-keygen -y -f /root/.ssh/id_rsa | tee /root/.ssh/id_rsa.pub
            ssh-keygen -y -f /root/.ssh/id_rsa | tee -a /root/.ssh/authorized_keys
            
            sec "Installing ceph and its dependencies"
            wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -
            echo deb https://download.ceph.com/debian-$ceph_release/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
            apt update
            apt -y --no-install-recommends install docker.io cephadm ceph-common nginx
            
            sec "Starting ceph bootstrap"
            cephadm bootstrap --mon-ip 192.168.40.10 --cluster-network 192.168.30.0/24 --ssh-user root --ssh-private-key /root/.ssh/id_rsa --ssh-public-key /root/.ssh/id_rsa.pub --skip-monitoring-stack --skip-dashboard
            
            sec "Deploy monitoring"
            ceph orch apply node-exporter
            ceph orch apply prometheus --placement="ceph-admin-node"
            ceph orch apply alertmanager --placement="ceph-admin-node"
            
            cat <<EOF > /root/grafana.yaml
            service_type: grafana
            spec:
              initial_admin_password: $password
            placement:
              hosts:
                - ceph-admin-node
            EOF
            ceph orch apply -i /root/grafana.yaml
            rm /root/grafana.yaml
            
            sec "Sleeping 60s"
            sleep 60
            
            sec "Pinning mgr and mon to the admin node"
            ceph orch apply mgr --placement="ceph-admin-node"
            ceph orch apply mon --placement="ceph-admin-node"
            
            sec "Setting configurations"
            ceph config set osd bluestore_block_wal_size 1073741824
            ceph config set osd bluestore_block_db_size 1073741824
            ceph config set osd osd_max_scrubs 2
            ceph config set osd osd_scrub_begin_hour 12
            ceph config set osd osd_scrub_end_hour 12
            ceph config set osd osd_recovery_op_priority 63
            ceph config set osd osd_client_op_priority 1
            ceph config set client.rgw.rgw rgw_op_thread_suicide_timeout 180
            
            sec "Sleeping 15s"
            sleep 15
            
            sec "Adding nodes"
            ceph orch host add ceph-monitor-node01
            ceph orch host add ceph-monitor-node02
            ceph orch host add ceph-monitor-node03
            ceph orch host add ceph-storage-node01
            ceph orch host add ceph-storage-node02
            ceph orch host add ceph-storage-node03
            
            sec "Sleeping 60s"
            sleep 60
            
            sec "Pinning mon daemons to monitor nodes"
            ceph orch apply mon --placement="ceph-monitor-node01,ceph-monitor-node02,ceph-monitor-node03"
            
            sec "Sleeping 120s"
            sleep 120
            
            sec "Adding OSDs"
            ceph orch daemon add osd ceph-storage-node01:/dev/vdb
            ceph orch daemon add osd ceph-storage-node02:/dev/vdb
            ceph orch daemon add osd ceph-storage-node03:/dev/vdb
            
            sec "Sleeping 60s"
            sleep 60
            
            sec "Deploying RGWs"
            ceph orch apply rgw rgw --port 8080 --placement="ceph-monitor-node01,ceph-monitor-node02,ceph-monitor-node03"
            
            sec "Deploying ceph dashboard"
            ceph mgr module enable dashboard
            ceph dashboard create-self-signed-cert
            ceph mgr module disable dashboard
            ceph mgr module enable dashboard
            ceph dashboard ac-user-create --enabled admin -i <(echo "$password") administrator "Admin User"
            ceph dashboard set-prometheus-api-host 'http://localhost:9095'
            ceph dashboard set-alertmanager-api-host 'http://localhost:9093'
            
            sec "Creating demo RBD volume"
            ceph osd pool create rbd replicated
            ceph osd pool set rbd size 2
            ceph osd pool application enable rbd rbd
            rbd create demo1 --size 10G
            rbd map demo1 --name client.admin
            mkfs.ext4 -m0 /dev/rbd/rbd/demo1
            mkdir /mnt/demo1
            mount /dev/rbd/rbd/demo1 /mnt/demo1
            dd if=/dev/urandom of=/mnt/demo1/test bs=1M count=5000 status=progress
            
          params:
            $username: { get_param: admin_username }
            $password: { get_param: admin_password }
            $ssh_priv_key: { get_attr: [ ssh_key_ceph_admin, private_key ] }
            $ceph_release: 17.2.0

        
  config_collection_for_monitors:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: config_base}
      - config: {get_resource: config_enable_root_ssh_login}
        
  config_collection_for_storages:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: config_base}
      - config: {get_resource: config_enable_root_ssh_login}
        
  config_collection_for_cdns:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: config_base}
      - config: {get_resource: config_enable_root_ssh_login}
      - config: {get_resource: config_install_rgw_proxy}
        
  config_collection_for_admins:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: config_base}
      - config: {get_resource: config_for_admins}
      
      
      
  backend_net:
    type: OS::Neutron::Net
    properties:
      port_security_enabled: true
      dns_domain: backend.
      
  backend_subnet:
    type: OS::Neutron::Subnet
    properties:
      network: { get_resource: backend_net }
      cidr: 192.168.30.0/24
      
  frontend_net:
    type: OS::Neutron::Net
    properties:
      port_security_enabled: true
      dns_domain: frontend.
      
  frontend_subnet:
    type: OS::Neutron::Subnet
    properties:
      network: { get_resource: frontend_net }
      cidr: 192.168.40.0/24
      
  frontend_router:
    type: OS::Neutron::Router
    properties:
      external_gateway_info:
        network: { get_param: public_net }
      
  frontend_router_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router: { get_resource: frontend_router }
      subnet: { get_resource: frontend_subnet }
     
     
      
  sec_grp_admin:
    type: OS::Neutron::SecurityGroup
    properties:
      name: ceph_sec_grp_admin
      rules:
        - direction: ingress
          protocol: tcp
          remote_ip_prefix: 192.168.255.254/32
          port_range_min: 22
          port_range_max: 22
          
  sec_grp_intra:
    type: OS::Neutron::SecurityGroup
    properties:
      name: ceph_sec_grp_intra
      rules:
        - direction: egress
          protocol: icmp
          remote_ip_prefix: 0.0.0.0/0
        - direction: egress
          protocol: tcp
          remote_ip_prefix: 0.0.0.0/0
        - direction: egress
          protocol: udp
          remote_ip_prefix: 0.0.0.0/0
        - direction: ingress
          remote_ip_prefix: 192.168.40.0/24
        - direction: ingress
          remote_ip_prefix: 192.168.30.0/24
   
  
  
  storage_volume01:
    type: OS::Cinder::Volume
    properties:
      size: { get_param: volume_size }
      
  storage_volume02:
    type: OS::Cinder::Volume
    properties:
      size: { get_param: volume_size }
      
  storage_volume03:
    type: OS::Cinder::Volume
    properties:
      size: { get_param: volume_size }
     
     
     
  storage_node01:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_storage_node01
      block_device_mapping:
        - device_name: vdb
          volume_id: { get_resource: storage_volume01 }
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.101
        - network: { get_resource: backend_net }
          fixed_ip: 192.168.30.101
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_storages }
     
  storage_node02:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_storage_node02
      block_device_mapping:
        - device_name: vdb
          volume_id: { get_resource: storage_volume02 }
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.102
        - network: { get_resource: backend_net }
          fixed_ip: 192.168.30.102
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_storages }
     
  storage_node03:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_storage_node03
      block_device_mapping:
        - device_name: vdb
          volume_id: { get_resource: storage_volume03 }
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.103
        - network: { get_resource: backend_net }
          fixed_ip: 192.168.30.103
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_storages }
     
     
  monitor_node01:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_monitor_node01
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.41
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_monitors }
     
  monitor_node02:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_monitor_node02
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.42
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_monitors }
     
  monitor_node03:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_monitor_node03
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.43
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_monitors }
     
     
  cdn_node01:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_cdn_node01
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.51
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_cdns }
     
  cdn_node02:
    type: OS::Nova::Server
    properties:
      key_name: { get_resource: ssh_key_ceph_admin }
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_cdn_node02
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.52
      security_groups: 
        - { get_resource: sec_grp_intra }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_cdns }
     
     
  admin_node:
    type: OS::Nova::Server
    properties:
      admin_pass: admin
      key_name: ssh_key_mgmt_admin
      image: ubuntu-focal
      flavor: m1.small
      name: ceph_admin_node
      networks:
        - network: { get_resource: frontend_net }
          fixed_ip: 192.168.40.10
        - network: { get_param: mgmt_net }
          fixed_ip: 192.168.255.30
      security_groups: 
        - { get_resource: sec_grp_intra }
        - { get_resource: sec_grp_admin }
      user_data_format: RAW
      user_data: { get_resource: config_collection_for_admins }
